{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the *Reuters-21578 Text Categorization Collection* from NTLK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import LazyCorpusLoader, CategorizedPlaintextCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters = LazyCorpusLoader('reuters', CategorizedPlaintextCorpusReader, '(training|test).*', cat_file='cats.txt', encoding='ISO-8859-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'id': reuters.fileids(), \n",
    "                   'text': [reuters.raw(i) for i in reuters.fileids()],\n",
    "                   'cats': [reuters.categories(i) for i in reuters.fileids()]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>cats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test/14826</td>\n",
       "      <td>ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RI...</td>\n",
       "      <td>[trade]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test/14828</td>\n",
       "      <td>CHINA DAILY SAYS VERMIN EAT 7-12 PCT GRAIN STO...</td>\n",
       "      <td>[grain]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test/14829</td>\n",
       "      <td>JAPAN TO REVISE LONG-TERM ENERGY DEMAND DOWNWA...</td>\n",
       "      <td>[crude, nat-gas]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test/14832</td>\n",
       "      <td>THAI TRADE DEFICIT WIDENS IN FIRST QUARTER\\n  ...</td>\n",
       "      <td>[corn, grain, rice, rubber, sugar, tin, trade]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test/14833</td>\n",
       "      <td>INDONESIA SEES CPO PRICE RISING SHARPLY\\n  Ind...</td>\n",
       "      <td>[palm-oil, veg-oil]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                               text  \\\n",
       "0  test/14826  ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RI...   \n",
       "1  test/14828  CHINA DAILY SAYS VERMIN EAT 7-12 PCT GRAIN STO...   \n",
       "2  test/14829  JAPAN TO REVISE LONG-TERM ENERGY DEMAND DOWNWA...   \n",
       "3  test/14832  THAI TRADE DEFICIT WIDENS IN FIRST QUARTER\\n  ...   \n",
       "4  test/14833  INDONESIA SEES CPO PRICE RISING SHARPLY\\n  Ind...   \n",
       "\n",
       "                                             cats  \n",
       "0                                         [trade]  \n",
       "1                                         [grain]  \n",
       "2                                [crude, nat-gas]  \n",
       "3  [corn, grain, rice, rubber, sugar, tin, trade]  \n",
       "4                             [palm-oil, veg-oil]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['test'] = df['id'].apply(lambda x: x.split('/')[0] == 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>cats</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test/14826</td>\n",
       "      <td>ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RI...</td>\n",
       "      <td>[trade]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test/14828</td>\n",
       "      <td>CHINA DAILY SAYS VERMIN EAT 7-12 PCT GRAIN STO...</td>\n",
       "      <td>[grain]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test/14829</td>\n",
       "      <td>JAPAN TO REVISE LONG-TERM ENERGY DEMAND DOWNWA...</td>\n",
       "      <td>[crude, nat-gas]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test/14832</td>\n",
       "      <td>THAI TRADE DEFICIT WIDENS IN FIRST QUARTER\\n  ...</td>\n",
       "      <td>[corn, grain, rice, rubber, sugar, tin, trade]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test/14833</td>\n",
       "      <td>INDONESIA SEES CPO PRICE RISING SHARPLY\\n  Ind...</td>\n",
       "      <td>[palm-oil, veg-oil]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                               text  \\\n",
       "0  test/14826  ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RI...   \n",
       "1  test/14828  CHINA DAILY SAYS VERMIN EAT 7-12 PCT GRAIN STO...   \n",
       "2  test/14829  JAPAN TO REVISE LONG-TERM ENERGY DEMAND DOWNWA...   \n",
       "3  test/14832  THAI TRADE DEFICIT WIDENS IN FIRST QUARTER\\n  ...   \n",
       "4  test/14833  INDONESIA SEES CPO PRICE RISING SHARPLY\\n  Ind...   \n",
       "\n",
       "                                             cats  test  \n",
       "0                                         [trade]  True  \n",
       "1                                         [grain]  True  \n",
       "2                                [crude, nat-gas]  True  \n",
       "3  [corn, grain, rice, rubber, sugar, tin, trade]  True  \n",
       "4                             [palm-oil, veg-oil]  True  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Tokens & Text Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split off the text title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title'] = df['text'].apply(lambda x: x.split('\\n')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['body'] = df['text'].apply(lambda x: ' '.join(\"\".join(x.split('\\n')[1:]).lower().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['body'] = df['body'].apply(lambda x: ' '.join([i for i in word_tokenize(x) if not i.isdigit()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentences'] = df['body'].apply(sent_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['n_sentences'] = df['sentences'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['max_sen_length'] = df['sentences'].apply(lambda x: max([len(word_tokenize(i)) for i in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_length'] = df['body'].apply(lambda x: len(word_tokenize(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need theses properties to use as matrices dimensions in the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TEXT_LENGTH = df['text_length'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_SENTS = df['n_sentences'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEN_LENGTH = df['max_sen_length'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max text length in words: 1661\n",
      "Max sent length in words: 428\n",
      "Max No sentences: 51\n"
     ]
    }
   ],
   "source": [
    "print('Max text length in words: {}\\nMax sent length in words: {}\\nMax No sentences: {}'\n",
    "      .format(MAX_TEXT_LENGTH, MAX_SEN_LENGTH, MAX_NUM_SENTS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad the sentences for use in the attention network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pad_sentences'] = df['sentences'].apply(lambda x: x + ([''] * (MAX_NUM_SENTS-len(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = MultiLabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_labels = one_hot.fit_transform(df['cats'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_LABELS = oh_labels.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, Input\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(df['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processer(raw_texts, maxlen=100):\n",
    "    tokens = tokenizer.texts_to_sequences(raw_texts)\n",
    "    return pad_sequences(tokens, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import GLOVE Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "f = open('glove.6B.100d.txt')\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, 100))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27545, 100)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import keras.backend as K\n",
    "from keras.layers import Layer, GRU, TimeDistributed, Bidirectional, Dense\n",
    "from keras import initializers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build an data-set in the format DOCUMENTS x SENTENCES x WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_data = np.stack(df['pad_sentences'].apply(lambda x: text_processer(x, MAX_SEN_LENGTH)).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10788, 51, 428)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_train, txt_test, x_train, x_test, y_train, y_test = train_test_split(df['body'].values, sen_data, oh_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network code was adapted from https://github.com/richliao/textClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttLayer(Layer):\n",
    "    def __init__(self, attention_dim):\n",
    "        self.init = initializers.get('normal')\n",
    "        self.supports_masking = True\n",
    "        self.attention_dim = attention_dim\n",
    "        super(AttLayer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)))\n",
    "        self.b = K.variable(self.init((self.attention_dim, )))\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)))\n",
    "        self.trainable_weights = [self.W, self.b, self.u]\n",
    "        super(AttLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # size of x :[batch_size, sel_len, attention_dim]\n",
    "        # size of u :[batch_size, attention_dim]\n",
    "        # uit = tanh(xW+b)\n",
    "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "\n",
    "        ait = K.exp(ait)\n",
    "\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = x * ait\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(tokenizer.word_index) + 1, 100,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEN_LENGTH,\n",
    "                            trainable=False, name='glove_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jordan/anaconda3/envs/attention_network/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jordan/anaconda3/envs/attention_network/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jordan/anaconda3/envs/attention_network/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jordan/anaconda3/envs/attention_network/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jordan/anaconda3/envs/attention_network/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence_input = Input(shape=(MAX_SEN_LENGTH,), dtype='int32', name='sentence_input')\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "l_lstm = Bidirectional(GRU(100, return_sequences=True))(embedded_sequences)\n",
    "l_att = AttLayer(100)(l_lstm)\n",
    "\n",
    "sentEncoder = Model(sentence_input, l_att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_input = Input(shape=(MAX_NUM_SENTS, MAX_SEN_LENGTH), dtype='int32', name='review_input')\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "l_lstm_sent = Bidirectional(GRU(100, return_sequences=True))(review_encoder)\n",
    "l_att_sent = AttLayer(100)(l_lstm_sent)\n",
    "preds = Dense(N_LABELS, activation='sigmoid')(l_att_sent)\n",
    "\n",
    "model = Model(review_input, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jordan/anaconda3/envs/attention_network/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jordan/anaconda3/envs/attention_network/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sentence_input (InputLayer)  (None, 428)               0         \n",
      "_________________________________________________________________\n",
      "glove_embedding (Embedding)  (None, 428, 100)          2754500   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 428, 200)          120600    \n",
      "_________________________________________________________________\n",
      "att_layer_1 (AttLayer)       (None, 200)               20200     \n",
      "=================================================================\n",
      "Total params: 2,895,300\n",
      "Trainable params: 140,800\n",
      "Non-trainable params: 2,754,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sentEncoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "review_input (InputLayer)    (None, 51, 428)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 51, 200)           2895300   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 51, 200)           180600    \n",
      "_________________________________________________________________\n",
      "att_layer_2 (AttLayer)       (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 90)                18090     \n",
      "=================================================================\n",
      "Total params: 3,114,190\n",
      "Trainable params: 359,690\n",
      "Non-trainable params: 2,754,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_example(n):\n",
    "    p = model.predict(x_test[n:n+1])[0]\n",
    "    top = sorted(list(zip(one_hot_a.classes_, p)), key=lambda x: x[1], reverse=True)\n",
    "    print(txt_test[n],'\\n')\n",
    "    print('Tagged as:', one_hot_a.classes_[np.nonzero(y_test[n])],'\\n')\n",
    "    for i in top[:4]:\n",
    "        print(\"{:25}:{:03.4f}\".format(i[0], i[1]))\n",
    "        \n",
    "    print('=========')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_out_model = Model(sentEncoder.input, sentEncoder.layers[2].output)\n",
    "word_ctx_0 = sentEncoder.layers[-1].get_weights()[0]\n",
    "word_ctx_1 = sentEncoder.layers[-1].get_weights()[1]\n",
    "word_ctx_2 = sentEncoder.layers[-1].get_weights()[2]\n",
    "\n",
    "def word_importance(n):\n",
    "    \"\"\"Extract tokens and corresponding attention weight from an indexed test message\"\"\"\n",
    "    example = x_test[n]\n",
    "    example = example[~np.all(example == 0, axis=1)]\n",
    "    word_enc = hidden_out_model.predict(example)\n",
    "    \n",
    "    u_watt = np.exp(np.dot(np.tanh(np.dot(word_enc, word_ctx_0)+word_ctx_1), word_ctx_2)[:,:,0])\n",
    "    u_watt = normalize(u_watt, axis=1, norm='l2')\n",
    "    \n",
    "    wrd_wts = []\n",
    "    \n",
    "    for i, j in enumerate(example):\n",
    "        wrds = np.trim_zeros(j)\n",
    "        toks = ([tokenizer.sequences_to_texts([wrds[k:k+1]])[0] for k in range(wrds.shape[0])])\n",
    "        wrd_wts.append([toks, u_watt[i, -len(toks):]])\n",
    "    \n",
    "    return wrd_wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(n):\n",
    "    \"\"\"Plot a bar chart of normalized attention weights with corresponding text labels\"\"\"\n",
    "    wts = word_importance(n)\n",
    "    r = list(range(max([len(i[0]) for i in wts])))\n",
    "    f, ax = plt.subplots(len(wts), sharex=True, sharey=True, figsize=(15, 2*len(wts)))\n",
    "    \n",
    "    if len(wts) > 1:\n",
    "        for i, j in enumerate(wts):\n",
    "            rects = ax[i].bar(r, j[1].tolist()+[0 for _ in range(len(r)-j[1].shape[0])], color='0.8')\n",
    "            ax[i].set_xticks([])\n",
    "            ax[i].set_yticks([])\n",
    "            for k, l in enumerate(j[0]):\n",
    "                ax[i].text(rects[k].get_x() + rects[k].get_width()/2., 0.5, l, ha='center', va='bottom')\n",
    "    \n",
    "    else:\n",
    "        for i, j in enumerate(wts):\n",
    "            rects = ax.bar(r, j[1].tolist()+[0 for _ in range(len(r)-j[1].shape[0])], color='0.8')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            for k, l in enumerate(j[0]):\n",
    "                ax.text(rects[k].get_x() + rects[k].get_width()/2., 0.5, l, ha='center', va='bottom')\n",
    "    \n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
